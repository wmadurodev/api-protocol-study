# gRPC vs REST Performance Analysis

## Executive Summary

This document explains the expected performance characteristics when comparing gRPC and REST for the `getUser` endpoint in this benchmarking project, and provides insight into why **REST is likely outperforming gRPC in your local tests**.

## Table of Contents
1. [Why REST is Beating gRPC Locally](#why-rest-is-beating-grpc-locally)
2. [Expected Results for getUser Endpoint](#expected-results-for-getuser-endpoint)
3. [When gRPC Should Beat REST](#when-grpc-should-beat-rest)
4. [When REST May Beat gRPC](#when-rest-may-beat-grpc)
5. [Optimization Recommendations](#optimization-recommendations)
6. [Testing Methodology](#testing-methodology)

---

## Why REST is Beating gRPC Locally

Based on your project architecture, there are several factors contributing to REST's superior performance in local tests:

### 1. **The Envoy Proxy Overhead**

**Critical Finding**: Your gRPC client communicates through an **Envoy proxy**, while REST connects directly to the backend.

```
REST Flow:
Browser â†’ REST Server (Port 8080)
[1 network hop, HTTP/1.1]

gRPC Flow:
Browser â†’ Envoy Proxy (Port 8090) â†’ gRPC Server (Port 9090)
[2 network hops, HTTP/1.1 â†’ gRPC-Web â†’ HTTP/2]
```

**Impact**: Each additional network hop adds latency:
- **Envoy processing**: Protocol translation, CORS handling, routing logic
- **Double serialization**: gRPC-Web wrapping on top of protobuf
- **Extra TCP connection**: Even on localhost, this adds microseconds
- **Memory copies**: Data copied between Envoy and gRPC server

**Evidence from your configuration** (`envoy.yaml:20`):
- Codec type: `AUTO` - Envoy must detect and convert protocols
- Filters chain: `grpc_web` â†’ `cors` â†’ `router` (3 filter stages)
- Timeout: 60s (indicates potential overhead concerns)

### 2. **gRPC-Web vs Native gRPC**

Your browser client uses **gRPC-Web**, not native gRPC, which introduces overhead:

| Feature | Native gRPC | gRPC-Web (Your Setup) |
|---------|-------------|----------------------|
| Protocol | HTTP/2 binary | HTTP/1.1 or HTTP/2 (through proxy) |
| Framing | gRPC framing | gRPC-Web framing (base64 encoded) |
| Streaming | Full bidirectional | Limited (unary + server streaming) |
| Browser support | âŒ None | âœ… Via Envoy/proxy |
| Overhead | Minimal | **20-30% larger payloads** |

**From your code** (`client/src/api/grpcClient.ts:27`):
```typescript
const GRPC_BASE_URL = 'http://localhost:8090';  // Envoy proxy, not direct gRPC
```

This means you're testing gRPC-Web performance, not pure gRPC performance.

### 3. **Payload Size for getUser is Small**

The `getUser` endpoint returns a single user object with 7 fields:
- `id` (int64)
- `username` (string)
- `email` (string)
- `firstName` (string)
- `lastName` (string)
- `createdAt` (timestamp)
- `isActive` (bool)

**Payload size estimation**:
- REST (JSON): ~200-250 bytes
- gRPC (Protobuf): ~120-150 bytes
- gRPC-Web (base64): ~160-200 bytes

**Analysis**: The 50-100 byte savings from protobuf are **negligible** compared to:
- Network latency: ~0.1-1ms on localhost
- Envoy overhead: ~1-5ms
- Protocol conversion: ~0.5-2ms

The serialization time difference for such small payloads is measured in **microseconds**, while the proxy overhead is measured in **milliseconds**.

### 4. **Single Request Pattern (No HTTP/2 Multiplexing Benefits)**

Your test likely executes requests one at a time for the `getUser` operation.

**gRPC's HTTP/2 multiplexing advantage requires**:
- Multiple concurrent requests over the **same connection**
- High request volume to amortize connection setup costs
- Long-lived connections to avoid handshake overhead

**Single request pattern**:
```
Test 1: Request â†’ Response â†’ [connection idle]
Test 2: Request â†’ Response â†’ [connection idle]
...
```

In this pattern, HTTP/2's main advantages (multiplexing, header compression) are wasted.

### 5. **Localhost Network Characteristics**

Testing on `localhost` eliminates the scenarios where gRPC excels:

| Network Condition | REST (HTTP/1.1) | gRPC (HTTP/2) | Winner |
|------------------|-----------------|---------------|---------|
| High latency (>100ms) | Slow (each request blocks) | Fast (multiplexing) | gRPC |
| Packet loss (>1%) | Needs retransmission | Better with single TCP | gRPC |
| Bandwidth constrained | Verbose JSON | Compact binary | gRPC |
| **Localhost (your tests)** | **Fast** | **Proxy overhead** | **REST** |

On localhost:
- Latency: ~0.01ms (negligible)
- Bandwidth: Effectively infinite
- Packet loss: 0%
- **Result**: gRPC's advantages disappear, overhead dominates

### 6. **Connection Setup Costs**

**REST overhead**:
- TCP handshake: ~0.1ms (localhost)
- HTTP connection: ~0.05ms
- **Total**: ~0.15ms

**gRPC-Web overhead**:
- TCP handshake to Envoy: ~0.1ms
- Envoy to gRPC server: ~0.1ms
- HTTP/2 handshake: ~0.2ms
- Protocol negotiation: ~0.1ms
- **Total**: ~0.5ms

Even with connection pooling, the dual-hop architecture penalizes gRPC.

### 7. **Spring Boot HTTP Configuration**

Both servers use Spring Boot 3.2.0, but with different starters:

**REST server** (`rest-server/pom.xml`):
- `spring-boot-starter-web`: Highly optimized Tomcat embedded server
- Mature HTTP/1.1 implementation with years of optimization
- Efficient JSON serialization with Jackson

**gRPC server** (`grpc-server/pom.xml`):
- `grpc-spring-boot-starter`: Relatively newer integration
- Additional abstraction layers
- Protocol buffer conversion overhead

---

## Expected Results for getUser Endpoint

### Local Testing (Your Current Scenario)

**Expected outcome**: âœ… **REST wins by 2-5x in response time**

Typical results you should see:

| Metric | REST | gRPC-Web | Difference |
|--------|------|----------|------------|
| Avg Response Time | **2-5ms** | 5-15ms | REST 2-3x faster |
| Min Response Time | 1-2ms | 3-8ms | REST 2-4x faster |
| Payload Size | 220 bytes | 180 bytes | gRPC 18% smaller |
| Connection Overhead | Low | High (Envoy) | REST much lower |

**Why**: Envoy proxy overhead + gRPC-Web encoding > JSON serialization savings

### Load Testing (High Concurrency)

**Expected outcome**: ðŸŸ¡ **Closer performance, gRPC may edge ahead at 100+ concurrent requests**

At 100+ concurrent requests:

| Metric | REST | gRPC-Web | Difference |
|--------|------|----------|------------|
| Avg Response Time | 50-100ms | 40-80ms | gRPC ~20% faster |
| P95 Response Time | 200ms | 120ms | gRPC ~40% faster |
| Connection pooling benefit | Moderate | High | gRPC wins |
| Throughput | 1000 req/s | 1500 req/s | gRPC ~50% higher |

**Why**: HTTP/2 multiplexing allows single connection to handle many concurrent requests, reducing overhead

---

## When gRPC Should Beat REST

gRPC will outperform REST in the following scenarios:

### 1. **Production Network Conditions**

**Scenario**: Deployed across real networks with latency and bandwidth constraints

```
Client (New York) â†’ Server (London)
- Latency: 70ms RTT
- Bandwidth: 10 Mbps
```

**Why gRPC wins**:
- HTTP/2 multiplexing eliminates head-of-line blocking
- Binary encoding reduces bandwidth usage by 30-60%
- Single TCP connection reduces handshake overhead
- Header compression with HPACK saves bandwidth

**Expected improvement**: 30-50% faster than REST

### 2. **High Request Volume & Concurrency**

**Scenario**: 1000+ requests per second from the same client

```javascript
// Concurrent requests benchmark
for (let i = 0; i < 1000; i++) {
  Promise.all([
    getUser(i),
    getUser(i+1),
    getUser(i+2),
    // ... 100 concurrent requests
  ]);
}
```

**Why gRPC wins**:
- **HTTP/2 multiplexing**: All requests share 1 TCP connection
  - REST: Opens 6-8 connections max (browser limit)
  - gRPC: Single connection, unlimited concurrent streams
- **Connection pooling efficiency**: Reuses existing HTTP/2 connection
- **Lower TCP overhead**: No connection setup/teardown for each request

**Expected improvement**: 50-100% faster at 100+ concurrent requests

### 3. **Complex, Nested Data Structures**

**Scenario**: Testing endpoints like `getUserOrders` which return nested data

```json
// REST response size
{
  "id": 1,
  "username": "john_doe",
  "orders": [
    {
      "id": 101,
      "orderDate": "2024-01-15T10:30:00Z",
      "totalAmount": 1299.99,
      "status": "SHIPPED",
      "items": [
        {"id": 1, "productName": "Laptop", "quantity": 1, "unitPrice": 999.99},
        {"id": 2, "productName": "Mouse", "quantity": 2, "unitPrice": 29.99}
      ]
    }
    // ... 10 more orders
  ]
}
```

**Size comparison**:
- REST (JSON): ~5-10 KB (verbose field names, string numbers)
- gRPC (Protobuf): ~2-4 KB (field numbers, binary encoding)
- **Savings**: 50-60% smaller payloads

**Why gRPC wins**:
- Repeated field names in JSON add significant overhead
- Protobuf uses field numbers (1 byte) vs field names (10+ bytes)
- Binary encoding of numbers is more efficient
- Timestamp encoding is compact

**Expected improvement**: 40-60% faster, especially over slow networks

### 4. **Streaming Operations**

**Scenario**: Real-time updates, server-sent events, long-running operations

```protobuf
// Hypothetical streaming RPC
rpc StreamUserUpdates(StreamRequest) returns (stream User);
```

**Why gRPC wins**:
- **Native bidirectional streaming** with HTTP/2
- REST requires:
  - Server-Sent Events (SSE) - server-to-client only
  - WebSockets - different protocol entirely
  - Long polling - inefficient
- **Flow control**: HTTP/2 provides automatic backpressure
- **Lower latency**: No HTTP header overhead per message

**Expected improvement**: 10x+ better for streaming use cases

### 5. **Microservices Communication (Server-to-Server)**

**Scenario**: Backend services calling each other (no browser involved)

```
Order Service â†’ User Service â†’ Payment Service
```

**Why gRPC wins**:
- **No Envoy proxy needed**: Direct gRPC communication
- **No gRPC-Web overhead**: Native HTTP/2 binary protocol
- **Type safety**: Protobuf contracts prevent runtime errors
- **Code generation**: Strongly-typed clients in all languages
- **Service mesh friendly**: Integrates well with Istio, Linkerd

**Expected improvement**:
- 50-70% faster than REST
- 3-5x smaller payload sizes
- Better error handling with status codes

### 6. **Mobile Clients on Cellular Networks**

**Scenario**: Mobile app on 3G/4G with:
- High latency: 100-300ms
- Limited bandwidth: 1-5 Mbps
- Packet loss: 1-5%
- Battery constraints

**Why gRPC wins**:
- **Smaller payloads**: Less data over cellular = faster + cheaper
- **Single connection**: Reduces radio state changes (saves battery)
- **Better retransmission**: HTTP/2 handles packet loss more efficiently
- **Header compression**: HPACK reduces overhead significantly

**Expected improvement**: 40-60% faster, 30-50% battery savings

### 7. **Polyglot Environments**

**Scenario**: Teams using multiple languages (Go, Java, Python, Node.js)

**Why gRPC wins**:
- **Automatic client generation**: `protoc` generates clients for 10+ languages
- **Type safety across languages**: Contract enforcement
- **Consistent serialization**: No JSON parsing differences
- **Better tooling**: grpcurl, grpc-gateway, reflection API

**Benefit**: Development velocity + fewer runtime errors

### 8. **High-Frequency Trading / Low-Latency Systems**

**Scenario**: Microsecond-level latency requirements

**Why gRPC wins**:
- **Binary protocol**: Faster parsing than JSON
- **Connection reuse**: No handshake overhead
- **Efficient serialization**: Protobuf is 5-10x faster than JSON parsing
- **Predictable performance**: No garbage from string parsing

**Expected improvement**: 20-40% lower P99 latency

---

## When REST May Beat gRPC

Don't assume gRPC is always better. REST has advantages in these scenarios:

### 1. **Browser-Based Applications (Without Direct Backend Control)**

**Your current scenario** applies here.

**When REST wins**:
- No proxy infrastructure needed
- Direct connection to backend
- Standard browser APIs (fetch, XMLHttpRequest)
- Easier debugging with browser DevTools
- No protocol translation overhead

### 2. **Simple CRUD Operations with Small Payloads**

**Scenario**: Basic `getUser`, `updateUser` with <1KB responses

**When REST wins**:
- Overhead of gRPC setup not justified
- JSON parsing is fast enough for small payloads
- HTTP caching works seamlessly (ETags, Cache-Control)
- Simpler architecture

### 3. **Public APIs with Unknown Clients**

**Scenario**: Public API consumed by third parties

**When REST wins**:
- Universal compatibility (curl, Postman, browsers)
- No special client libraries needed
- Well-understood by all developers
- OpenAPI/Swagger for documentation
- HTTP status codes are familiar

### 4. **Heavy Use of HTTP Caching**

**Scenario**: Mostly read operations with cacheable responses

```http
GET /api/users/1
Cache-Control: max-age=3600
ETag: "33a64df551425fcc55e4d42a148795d9f25f89d4"
```

**When REST wins**:
- HTTP caches (Varnish, CDN, browser) work out-of-box
- gRPC caching requires custom implementation
- Can avoid backend calls entirely with 304 Not Modified

### 5. **Low-Frequency, Interactive Operations**

**Scenario**: Admin dashboards, occasional API calls

**When REST wins**:
- Connection setup cost amortized over time is irrelevant
- Simplicity > performance
- Easier to debug and monitor
- Standard tooling (curl, wget, Postman)

---

## Optimization Recommendations

### To Make gRPC Faster in Your Tests

#### Option 1: **Remove Envoy Proxy (Server-to-Server Testing)**

Test with a backend client instead of browser:

```bash
# Python gRPC client (direct connection)
cd python-grpc-client
python client.py
```

**Expected result**: gRPC should now be 2-3x faster than REST

#### Option 2: **Test with High Concurrency**

Modify your test to run 100+ concurrent requests:

```typescript
// In performanceTester.ts
const promises = [];
for (let i = 0; i < 100; i++) {
  promises.push(grpcApi.getUser(Math.floor(Math.random() * 10000)));
}
await Promise.all(promises);
```

**Expected result**: gRPC throughput should surpass REST

#### Option 3: **Test Complex Endpoints**

Use `getUserOrders` or `listUsers` instead of `getUser`:

```typescript
// Test with larger payloads
await grpcApi.getUserOrders(1);  // Returns array of orders with items
await grpcApi.listUsers(0, 100); // Returns 100 users
```

**Expected result**: gRPC payload advantage becomes visible (30-50% smaller)

#### Option 4: **Simulate Network Latency**

Add artificial latency to simulate production:

```bash
# Linux: Add 50ms latency
sudo tc qdisc add dev lo root netem delay 50ms

# macOS: Use Network Link Conditioner
# Windows: Use clumsy or NetLimiter
```

**Expected result**: gRPC's multiplexing should show 20-30% improvement

#### Option 5: **Use Native gRPC (Skip Browser)**

Create a Node.js gRPC client using `@grpc/grpc-js`:

```javascript
// Direct gRPC, no gRPC-Web
const grpc = require('@grpc/grpc-js');
const client = new UserServiceClient('localhost:9090',
  grpc.credentials.createInsecure());
```

**Expected result**: gRPC should be 30-50% faster than browser gRPC-Web

### To Make REST Faster

#### Option 1: **Enable HTTP/2 in REST Server**

Spring Boot supports HTTP/2:

```yaml
# application.yml
server:
  http2:
    enabled: true
```

**Expected result**: REST gets multiplexing benefits, narrows gap with gRPC

#### Option 2: **Use HTTP Keep-Alive**

Ensure persistent connections:

```java
// Already enabled by default in Spring Boot
// But verify in production environments
```

---

## Testing Methodology

### Recommended Benchmark Setup

To get accurate comparisons:

1. **Separate Concerns**:
   - Test browser gRPC-Web vs REST (apples-to-apples)
   - Test native gRPC vs REST separately (server-to-server)

2. **Vary Request Patterns**:
   ```typescript
   // Single request
   await api.getUser(1);

   // Burst of 10
   await Promise.all([...Array(10)].map((_, i) => api.getUser(i)));

   // Sustained load (100 req/s for 60s)
   ```

3. **Vary Payload Sizes**:
   ```typescript
   getUser(1);           // ~200 bytes
   getUserOrders(1);     // ~5 KB
   listUsers(0, 100);    // ~20 KB
   bulkCreateUsers(500); // ~50 KB
   ```

4. **Vary Network Conditions**:
   - Localhost (your current tests)
   - Simulated latency (50ms, 100ms, 200ms)
   - Simulated packet loss (1%, 2%, 5%)
   - Bandwidth throttling (10 Mbps, 1 Mbps)

5. **Measure the Right Metrics**:
   ```typescript
   {
     responseTime: number;      // End-to-end latency
     serializationTime: number; // Time to encode/decode
     networkTime: number;       // Wire time
     payloadSize: number;       // Bytes over the wire
     throughput: number;        // Requests per second
     cpuUsage: number;         // Server CPU %
     memoryUsage: number;      // Server memory MB
   }
   ```

### Load Testing with `ghz`

For accurate gRPC benchmarking:

```bash
# Install ghz
go install github.com/bojand/ghz/cmd/ghz@latest

# Benchmark getUser with 100 concurrent connections
ghz --insecure \
  --proto ./shared/proto/user_service.proto \
  --call api.performance.UserService/GetUser \
  -d '{"user_id": 1}' \
  -c 100 \
  -n 10000 \
  localhost:9090

# Compare with REST using Apache Bench
ab -n 10000 -c 100 http://localhost:8080/api/users/1
```

### Statistical Significance

Run multiple iterations:

```typescript
const iterations = 1000;
const results = [];

for (let i = 0; i < iterations; i++) {
  const start = performance.now();
  await api.getUser(randomUserId());
  results.push(performance.now() - start);
}

// Calculate statistics
const mean = results.reduce((a, b) => a + b) / results.length;
const p50 = percentile(results, 50);
const p95 = percentile(results, 95);
const p99 = percentile(results, 99);
const stdDev = calculateStdDev(results);
```

---

## Conclusion

### Why Your Local Tests Show REST Winning

1. **Envoy proxy adds 2-5ms overhead** (largest factor)
2. **gRPC-Web has 20-30% encoding overhead** vs native gRPC
3. **Small payloads** (200 bytes) don't benefit from binary encoding
4. **Single request pattern** wastes HTTP/2 multiplexing
5. **Localhost eliminates network latency** where gRPC shines
6. **No connection reuse** negates gRPC's persistent connection advantage

### When to Use gRPC

Choose gRPC when you have:
- High request volume (100+ req/s)
- Concurrent requests from same client
- Large or complex payloads (>1 KB)
- Production network with latency (>50ms)
- Microservices communication
- Streaming requirements
- Polyglot environment
- Mobile clients on cellular

### When to Use REST

Choose REST when you have:
- Browser-based applications (without proxy infrastructure)
- Simple CRUD operations
- Small payloads (<1 KB)
- Public APIs
- Heavy caching requirements
- Low request frequency
- Developer familiarity priority

### The Verdict for Your Project

**Current findings**: âœ… REST is correctly winning in your local browser tests

**This is expected** because:
- You're testing gRPC-Web (not native gRPC) through Envoy proxy
- Small payloads on localhost don't showcase gRPC's strengths
- Single request pattern doesn't leverage HTTP/2 multiplexing

**To see gRPC win**, test with:
1. Native gRPC clients (Python script, Node.js with @grpc/grpc-js)
2. High concurrency (100+ concurrent requests)
3. Complex endpoints (getUserOrders, listUsers with large page sizes)
4. Simulated network latency (50-100ms)
5. Sustained load (1000+ req/s)

Both protocols are optimized for different scenarios - your benchmarking project correctly demonstrates that **there is no universal winner**.
